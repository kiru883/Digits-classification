{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_engineering.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOImDTV8jHCdw22p+Dxt73b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiru883/Digits-classification/blob/master/feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agAx8fROcqmy",
        "colab_type": "code",
        "outputId": "7b0f9661-97d2-4b4d-edee-40a29f7cf7a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import TimeSeriesSplit, train_test_split, KFold\n",
        "from sklearn.feature_selection import RFECV"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk84hK2pjZC-",
        "colab_type": "text"
      },
      "source": [
        "# Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOtPbr0idRx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####LOAD DATASETS\n",
        "# train\n",
        "data_trainTR = pd.read_csv(\"/content/gdrive/My Drive/frauds_datasets/train_transaction.csv\")\n",
        "data_trainID = pd.read_csv(\"/content/gdrive/My Drive/frauds_datasets/train_identity.csv\")\n",
        "\n",
        "# test\n",
        "data_testTR = pd.read_csv(\"/content/gdrive/My Drive/frauds_datasets/test_transaction.csv\")\n",
        "data_testID = pd.read_csv(\"/content/gdrive/My Drive/frauds_datasets/test_identity.csv\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOEIUo-PjTiI",
        "colab_type": "text"
      },
      "source": [
        "# Memory usage reduction function and etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkaII_HMgHR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimize memory\n",
        "def reduce_mem_usage(df):\n",
        "    mem_usage_before = np.around(df.memory_usage().sum() / 1028**2)\n",
        "    print(f\"Memory usage before: {mem_usage_before} MB\")\n",
        "\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'float':\n",
        "            mn, mx = df[column].min(), df[column].max()\n",
        "            if mn > -2147483648 or mx < 2147483648:\n",
        "                df[column] = df[column].astype('float32')\n",
        "\n",
        "        elif df[column].dtype == 'int':\n",
        "            mn, mx = df[column].min(), df[column].max()\n",
        "            if mn > -128 or mx < 127:\n",
        "                df[column] = df[column].astype('int8')\n",
        "            elif mn > -32000 or mx < 32000:\n",
        "                df[column] = df[column].astype('int16')\n",
        "            elif mn > -2147483648 or mx < 2147483648:\n",
        "                df[column] = df[column].astype('int32')\n",
        "\n",
        "        elif df[column].dtype == 'object':\n",
        "            df[column] = df[column].astype('category')\n",
        "\n",
        "    mem_usage_after = np.around(df.memory_usage().sum() / 1028**2)\n",
        "    print(f\"Memory usage after: {mem_usage_after} MB\")\n",
        "    print(f\"Optimization: {np.around(100*(1 - mem_usage_after/mem_usage_before))}%\")\n",
        "\n",
        "    return df\n",
        "\n",
        "#check features on dependence on time\n",
        "def time_dependence(feature):\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saMr3JXpuYuu",
        "colab_type": "text"
      },
      "source": [
        "# Main pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APSMQ9PMgP6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with all my hypotises\n",
        "def pipeline(data_TR, data_ID):\n",
        "    #useful functions\n",
        "    def decimal_places(x):\n",
        "        decimal_str = str(x)[str(x).find(\".\") +1:]\n",
        "        if decimal_str == \"0\":\n",
        "            return 0\n",
        "        decimal_len = len(str(int(decimal_str[::-1])))\n",
        "        return decimal_len if decimal_len < 5 else 5\n",
        "    \n",
        "    def email_map(email):\n",
        "        mapping= {'frontier.com':'frontier','frontiernet.net':'frontier','gmail':'gmail','gmail.com':'gmail','hotmail.co.uk':'hotmail','hotmail.com':'Microsoft','hotmail.de':'Microsoft',\n",
        "            'hotmail.es':'Microsoft','hotmail.fr':'Microsoft','icloud.com':'Apple','live.com':'Microsoft','live.com.mx':'Microsoft','live.fr':'Microsoft','mac.com':'Apple',\n",
        "            'netzero.com':'Netzero','netzero.net':'Netzero','outlook.com':'Microsoft','outlook.es':'Microsoft', 'yahoo.co.jp':'Yahoo','yahoo.co.uk':'Yahoo','yahoo.com':'Yahoo',\n",
        "            'yahoo.com.mx':'Yahoo','yahoo.de':'Yahoo','yahoo.es':'Yahoo','yahoo.fr':'Yahoo','ymail.com':'Yahoo', 'scranton.edu':'Scranton'}\n",
        "        if email in mapping.keys():\n",
        "            return mapping[email]\n",
        "        elif pd.isnull(email):\n",
        "            return 'NAN'\n",
        "        else:\n",
        "            return 'other'\n",
        "\n",
        "    def parse_id30(x):\n",
        "        devices_30_list = ['windows', 'ios', 'mac', 'android', 'linux']\n",
        "        if pd.isnull(x): \n",
        "            return 'NAN'\n",
        "        elif x.split()[0].lower() in devices_30_list: \n",
        "            return x.split()[0].lower()\n",
        "        else:\n",
        "            return 'NAN'\n",
        "        \n",
        "    def parse_id31(x):\n",
        "        devices_30_set = {'chrome', 'safari', 'ie', 'edge', 'firefox'}\n",
        "        if pd.isnull(x): \n",
        "            return 'NAN'\n",
        "        result = list(devices_30_set & set(x.split()))\n",
        "        if len(result) == 0:\n",
        "            return 'other'\n",
        "        else:\n",
        "            return result[0]\n",
        "        \n",
        "    def parse_id33(x):\n",
        "        devices_33_list = ['1334x750', '2436x1125', '1366x768', '1920x1080', '2208x1242']\n",
        "        if pd.isnull(x):\n",
        "            return 'NAN'\n",
        "        if x in devices_33_list:\n",
        "            return x\n",
        "        else:\n",
        "            return 'other'\n",
        "        \n",
        "    def parse_deviceinfo(x):\n",
        "        devices_info_list = ['windows', 'macos', 'ios', 'trident/7.0']\n",
        "        if pd.isnull(x):\n",
        "            return 'NAN'\n",
        "        x = x.split()[0].lower()\n",
        "        if x in devices_info_list:\n",
        "            return x\n",
        "        else:\n",
        "            return 'other'\n",
        "\n",
        "    df = pd.concat([data_TR.set_index('TransactionID'), data_ID.set_index('TransactionID')], axis=1).reset_index()\n",
        "    del data_TR, data_ID\n",
        "\n",
        "    #main pipeline\n",
        "    df['month'] = df['TransactionDT'] // (86400 * 30)\n",
        "    # transactionAmt features\n",
        "    df['TransactionAmt'] = df['TransactionAmt'].fillna(-999)\n",
        "    df['transaction_month'] = df.groupby(['month'])['TransactionAmt'].transform('mean') - df['TransactionAmt']\n",
        "    df['trans_meanq'] = pd.qcut(df['TransactionAmt'].median() - df['TransactionAmt'], [0.1, 0.3, 0.5, 0.7, 0.9], labels=False).fillna(-1)\n",
        "    df['trans_std_negative'] = np.where((df['transaction_month'] < 0), 1, 0)\n",
        "    df['transaction_std'] = df['transaction_month'] / df.groupby(['month'])['TransactionAmt'].transform('std')\n",
        "    df['transaction_stdq'] = (df['TransactionAmt'].median() - df['TransactionAmt']) / df['TransactionAmt'].std()\n",
        "    df['transaction_stdq'] = pd.qcut(df['transaction_stdq'], [0.1, 0.3, 0.5, 0.7, 0.9], labels=False).fillna(-1)\n",
        "    df['transaction_digits'] = df['TransactionAmt'].map(decimal_places)\n",
        "    df['transaction_count'] = df['TransactionAmt'].map(df['TransactionAmt'].value_counts())\n",
        "    df['transaction_count'] = pd.qcut(df['transaction_count'], [0.1, 0.3, 0.5, 0.7, 0.9], labels=False).fillna(-1)\n",
        "    high_tr = df['TransactionAmt'].quantile([0.9]).to_list()[0]\n",
        "    lower_tr = df['TransactionAmt'].quantile([0.1]).to_list()[0]\n",
        "    df['outlier'] = np.where((df['TransactionAmt'] > high_tr) | (df['TransactionAmt'] < lower_tr), 1, 0)\n",
        "    df['trans_hour'] = (df['TransactionDT'] % 86400) // 3600\n",
        "    df['trans_cent'] = df['TransactionAmt'] % 1\n",
        "\n",
        "    #ProductCd\n",
        "    df['ProductCD'] = df['ProductCD'].fillna('NAN')\n",
        "    df['prod_stdq'] = pd.qcut(df.groupby([\"ProductCD\"])['TransactionAmt'].transform('median') - df['TransactionAmt'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "    df['ProductCD'] = LabelEncoder().fit_transform(df['ProductCD'])\n",
        "    \n",
        "    #card1\n",
        "    df['card1'] = df['card1'].fillna(-999)\n",
        "    df['card1_count'] = df['card1'].map(df['card1'].value_counts())\n",
        "    df['card1_count_q'] = pd.qcut(df['card1_count'], [0.05, 0.2, 0.4, 0.6, 0.8, 0.95], labels=False).fillna(-1)\n",
        "    df['card1_frequency'] = df['card1'].map(df['card1'].value_counts() / df['card1'].shape[0])\n",
        "    df['card1_frequency'] = pd.qcut(df['card1_frequency'], [0.05, 0.2, 0.4, 0.6, 0.8, 0.95], labels=False).fillna(-1)\n",
        "    df['trans_card1_mean'] = df.groupby(['card1'])['TransactionAmt'].transform('mean') - df['TransactionAmt']\n",
        "    df['trans_card1_mean_rel'] = df.groupby(['card1'])['TransactionAmt'].transform('mean') / df['TransactionAmt']\n",
        "    df['card1_mean'] = df.groupby(['month'])['card1'].transform('mean') - df['card1']\n",
        "    df['card1_std'] = df.groupby(['month'])['card1'].transform('std') / df['card1_mean'] \n",
        "    df['card1_addr1_mean'] = df['card1'].astype('str') + '_' + df['addr1'].astype('str')\n",
        "    df['card1_addr1_mean'] = df.groupby(['card1_addr1_mean'])['TransactionAmt'].transform('mean') - df['TransactionAmt']\n",
        "\n",
        "    #card2\n",
        "    df['card2'] = df['card2'].fillna(-999)\n",
        "    df['card2_count'] = df['card2'].map(df['card2'].value_counts())\n",
        "    df['card2_out'] = pd.qcut(df['card2_count'], [0.05, 0.15, 0.85, 0.95], labels=False).fillna(-1)\n",
        "    df['card2_q'] = pd.qcut(df['card2_count'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "\n",
        "    #card3\n",
        "    df['card3'] = df['card3'].fillna(-999)\n",
        "    df['card3_o'] = np.where((df['card3'] < df['card3'].quantile(0.1)) | (df['card3'] > df['card3'].quantile(0.9)), 1, 0)\n",
        "    \n",
        "    #card4\n",
        "    df['card4'] = df['card4'].fillna('NAN')\n",
        "    df['card4_count'] = df['card4'].map(df['card4'].value_counts())\n",
        "    df['trans_card_mean'] = df.groupby(['card4'])['TransactionAmt'].transform('mean') - df['TransactionAmt']\n",
        "    df['trans_card_mean_q'] = pd.qcut(df['trans_card_mean'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "    df['trans_card_std'] = df.groupby(['card4'])['TransactionAmt'].transform('std') / df['trans_card_mean']\n",
        "    df['trans_card_std_q'] = pd.qcut(df['trans_card_std'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "    df['card4_trans_mean'] = df['TransactionAmt'].astype('str') + '_' + df['card4'].astype('str')\n",
        "    df['card4_trans_mean'] = df.groupby(['card4_trans_mean'])['TransactionAmt'].transform('mean') - df['TransactionAmt']\n",
        "    df['card4'] = LabelEncoder().fit_transform(df['card4'])\n",
        "\n",
        "    #card5\n",
        "    df['card5'] = df['card5'].fillna(-999)\n",
        "    df['card5_o'] = np.where(((df['card5'] < df['card5'].quantile(0.05)) | (df['card5'] > df['card5'].quantile(0.95))), 1, 0)\n",
        "\n",
        "    #card6\n",
        "    df['card6'] = df['card6'].fillna('NAN')\n",
        "    df['trans_card_med'] = df.groupby(['card6'])['TransactionAmt'].transform('median') - df['TransactionAmt']\n",
        "    df['card6'] = LabelEncoder().fit_transform(df['card6'])\n",
        "\n",
        "    #addr1\n",
        "    df['addr1'] = df['addr1'].fillna(-999)\n",
        "    df['addr1_isnull'] = df['addr1'].isnull().astype(int)\n",
        "    df['addr1_frequency'] = df['addr1'].map(df['addr1'].value_counts())\n",
        "    df['addr_trans'] = df.groupby(['addr1'])['TransactionAmt'].transform('median') - df['TransactionAmt']\n",
        "    df['addr_trans_q'] = pd.qcut(df['addr_trans'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "    df['addr_card_med'] = df.groupby(['addr1'])['card1'].transform('median') - df['card1']\n",
        "    df['addr_card'] = df.groupby(['addr1'])['card1'].transform('median') - df['card1']\n",
        "    df['addr_card_q'] = pd.qcut(df['addr_card'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "    df['addrcard'] = df['addr1'].astype(str) + \"_\" + df['card1'].astype(str)\n",
        "    df['addrcard'] = LabelEncoder().fit_transform(df['addrcard'])\n",
        "\n",
        "    #addr2 & dist1 & dist2\n",
        "    df['addr2'] = df['addr2'].fillna(-999)\n",
        "    df['dist1'] = df['dist1'].fillna(-999)\n",
        "    df['dist2'] = df['dist2'].fillna(-999)\n",
        "\n",
        "    #p_emaildomain\n",
        "    df['P_emaildomain'] = LabelEncoder().fit_transform(df['P_emaildomain'].map(email_map))\n",
        "\n",
        "    #r_emaildomain\n",
        "    df['R_emaildomain'] = LabelEncoder().fit_transform(df['R_emaildomain'].map(email_map))\n",
        "\n",
        "    #Cs\n",
        "    for Ci in range(1, 15):\n",
        "        c = \"C\" + str(Ci)\n",
        "        df[c] = df[c].fillna(-999)\n",
        "        df[c + '_trans_div'] = df.groupby([c])['TransactionAmt'].transform('median') - df['TransactionAmt']\n",
        "        df[c + '_trans_div_q'] = pd.qcut(df[c + '_trans_div'], [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], labels=False).fillna(-1)\n",
        "\n",
        "    #Ds\n",
        "    for Di in range(1, 16):\n",
        "        d = \"D\" + str(Di)\n",
        "        df[d] = df[d].fillna(-999)\n",
        "    \n",
        "    #Ms\n",
        "    for Mi in range(1, 10):\n",
        "        m = \"M\" + str(Mi)\n",
        "        df[m] = df[m].fillna(\"NAN\")\n",
        "        df[m] = LabelEncoder().fit_transform(df[m])\n",
        "\n",
        "    #Vs\n",
        "    for Vi in range(1, 340):\n",
        "        v = \"V\" + str(Vi)\n",
        "        df[v] = df[v].fillna(-999)\n",
        "\n",
        "    #ids\n",
        "    for Ii in range(1, 39):\n",
        "        i = \"id_0\" if Ii < 10 else \"id_\"\n",
        "        i += str(Ii)\n",
        "        if df[i].dtype == 'object':\n",
        "            if i == \"id_30\":\n",
        "                df[i] = df[i].map(parse_id30)\n",
        "            elif i == \"id_31\":\n",
        "                df[i] = df[i].map(parse_id31)\n",
        "            elif i == 'id_33':\n",
        "                df[i] = df[i].map(parse_id33)\n",
        "            df[i] = df[i].fillna('NAN')\n",
        "            df[i] = LabelEncoder().fit_transform(df[i])\n",
        "        else:\n",
        "            df[i] = df[i].fillna(-999)\n",
        "\n",
        "    #test functions\n",
        "    features = ['TransactionAmt', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'D15', 'C13', 'addr1', 'dist1', 'R_emaildomain', 'M2', 'M4', 'trans_hour']\n",
        "    features_combinations = list(itertools.combinations(features, 2))\n",
        "    for f1, f2 in features_combinations:\n",
        "        feature_name = f1 + '_' + f2\n",
        "        df[feature_name] = df[f1].astype('str') + '_' + df[f2].astype('str')\n",
        "        df[feature_name] = df[feature_name].map(df[feature_name].value_counts())\n",
        "\n",
        "    #deviceType & deviceInfo\n",
        "    df['DeviceType'] = LabelEncoder().fit_transform(df['DeviceType'].fillna(\"NAN\"))\n",
        "    df['DeviceInfo'] = LabelEncoder().fit_transform(df['DeviceInfo'].map(parse_deviceinfo))\n",
        "\n",
        "    df.sort_values(by=[\"TransactionDT\"], ascending=True).reset_index()\n",
        "    df = df.drop([\"TransactionDT\", \"TransactionID\", \"month\"], axis=1)\n",
        "\n",
        "    gc.collect()\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdH20PO2Mixa",
        "colab_type": "text"
      },
      "source": [
        "# RFE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGaxVc-2W9mE",
        "colab_type": "code",
        "outputId": "b86fc636-b027-4427-d803-8630c2e5596a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# prepare data, X, y\n",
        "test = pipeline(data_trainTR.drop(['isFraud'], axis=1), data_trainID)\n",
        "test = reduce_mem_usage(test)\n",
        "X = test.to_numpy()\n",
        "y = data_trainTR['isFraud'].to_numpy().reshape(-1, 1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage before: 2696.0 MB\n",
            "Memory usage after: 1098.0 MB\n",
            "Optimization: 59.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roOxLlZuaBOJ",
        "colab_type": "code",
        "outputId": "2569eaf8-0e06-4846-8d73-1cbe8ca6f56e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#RFE estimator, step = 20\n",
        "clf = lgb.LGBMClassifier(\n",
        "    num_leaves = 400,\n",
        "    n_estimators = 300,\n",
        "    metrics = 'auc',\n",
        "    objective = 'binary'\n",
        ")\n",
        "\n",
        "#RFECV\n",
        "rfecv = RFECV(\n",
        "    estimator=clf,\n",
        "    step=20,\n",
        "    cv=TimeSeriesSplit(),\n",
        "    scoring='roc_auc',\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "rfecv.fit(X, y)\n",
        "\n",
        "features = [x for x in test.columns[rfecv.ranking_ == 1]]\n",
        "print(f\"Number of features: {len(features)}\")\n",
        "print(\"\\nFeatures:\\n\", features)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting estimator with 603 features.\n",
            "Fitting estimator with 583 features.\n",
            "Fitting estimator with 563 features.\n",
            "Fitting estimator with 543 features.\n",
            "Fitting estimator with 523 features.\n",
            "Fitting estimator with 503 features.\n",
            "Fitting estimator with 483 features.\n",
            "Fitting estimator with 463 features.\n",
            "Fitting estimator with 443 features.\n",
            "Fitting estimator with 423 features.\n",
            "Fitting estimator with 403 features.\n",
            "Fitting estimator with 383 features.\n",
            "Fitting estimator with 363 features.\n",
            "Fitting estimator with 343 features.\n",
            "Fitting estimator with 323 features.\n",
            "Fitting estimator with 303 features.\n",
            "Fitting estimator with 283 features.\n",
            "Fitting estimator with 263 features.\n",
            "Fitting estimator with 243 features.\n",
            "Fitting estimator with 223 features.\n",
            "Fitting estimator with 203 features.\n",
            "Fitting estimator with 183 features.\n",
            "Fitting estimator with 163 features.\n",
            "Fitting estimator with 143 features.\n",
            "Fitting estimator with 123 features.\n",
            "Fitting estimator with 103 features.\n",
            "Fitting estimator with 83 features.\n",
            "Fitting estimator with 63 features.\n",
            "Fitting estimator with 43 features.\n",
            "Fitting estimator with 23 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 603 features.\n",
            "Fitting estimator with 583 features.\n",
            "Fitting estimator with 563 features.\n",
            "Fitting estimator with 543 features.\n",
            "Fitting estimator with 523 features.\n",
            "Fitting estimator with 503 features.\n",
            "Fitting estimator with 483 features.\n",
            "Fitting estimator with 463 features.\n",
            "Fitting estimator with 443 features.\n",
            "Fitting estimator with 423 features.\n",
            "Fitting estimator with 403 features.\n",
            "Fitting estimator with 383 features.\n",
            "Fitting estimator with 363 features.\n",
            "Fitting estimator with 343 features.\n",
            "Fitting estimator with 323 features.\n",
            "Fitting estimator with 303 features.\n",
            "Fitting estimator with 283 features.\n",
            "Fitting estimator with 263 features.\n",
            "Fitting estimator with 243 features.\n",
            "Fitting estimator with 223 features.\n",
            "Fitting estimator with 203 features.\n",
            "Fitting estimator with 183 features.\n",
            "Fitting estimator with 163 features.\n",
            "Fitting estimator with 143 features.\n",
            "Fitting estimator with 123 features.\n",
            "Fitting estimator with 103 features.\n",
            "Fitting estimator with 83 features.\n",
            "Fitting estimator with 63 features.\n",
            "Fitting estimator with 43 features.\n",
            "Fitting estimator with 23 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 603 features.\n",
            "Fitting estimator with 583 features.\n",
            "Fitting estimator with 563 features.\n",
            "Fitting estimator with 543 features.\n",
            "Fitting estimator with 523 features.\n",
            "Fitting estimator with 503 features.\n",
            "Fitting estimator with 483 features.\n",
            "Fitting estimator with 463 features.\n",
            "Fitting estimator with 443 features.\n",
            "Fitting estimator with 423 features.\n",
            "Fitting estimator with 403 features.\n",
            "Fitting estimator with 383 features.\n",
            "Fitting estimator with 363 features.\n",
            "Fitting estimator with 343 features.\n",
            "Fitting estimator with 323 features.\n",
            "Fitting estimator with 303 features.\n",
            "Fitting estimator with 283 features.\n",
            "Fitting estimator with 263 features.\n",
            "Fitting estimator with 243 features.\n",
            "Fitting estimator with 223 features.\n",
            "Fitting estimator with 203 features.\n",
            "Fitting estimator with 183 features.\n",
            "Fitting estimator with 163 features.\n",
            "Fitting estimator with 143 features.\n",
            "Fitting estimator with 123 features.\n",
            "Fitting estimator with 103 features.\n",
            "Fitting estimator with 83 features.\n",
            "Fitting estimator with 63 features.\n",
            "Fitting estimator with 43 features.\n",
            "Fitting estimator with 23 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 603 features.\n",
            "Fitting estimator with 583 features.\n",
            "Fitting estimator with 563 features.\n",
            "Fitting estimator with 543 features.\n",
            "Fitting estimator with 523 features.\n",
            "Fitting estimator with 503 features.\n",
            "Fitting estimator with 483 features.\n",
            "Fitting estimator with 463 features.\n",
            "Fitting estimator with 443 features.\n",
            "Fitting estimator with 423 features.\n",
            "Fitting estimator with 403 features.\n",
            "Fitting estimator with 383 features.\n",
            "Fitting estimator with 363 features.\n",
            "Fitting estimator with 343 features.\n",
            "Fitting estimator with 323 features.\n",
            "Fitting estimator with 303 features.\n",
            "Fitting estimator with 283 features.\n",
            "Fitting estimator with 263 features.\n",
            "Fitting estimator with 243 features.\n",
            "Fitting estimator with 223 features.\n",
            "Fitting estimator with 203 features.\n",
            "Fitting estimator with 183 features.\n",
            "Fitting estimator with 163 features.\n",
            "Fitting estimator with 143 features.\n",
            "Fitting estimator with 123 features.\n",
            "Fitting estimator with 103 features.\n",
            "Fitting estimator with 83 features.\n",
            "Fitting estimator with 63 features.\n",
            "Fitting estimator with 43 features.\n",
            "Fitting estimator with 23 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 603 features.\n",
            "Fitting estimator with 583 features.\n",
            "Fitting estimator with 563 features.\n",
            "Fitting estimator with 543 features.\n",
            "Fitting estimator with 523 features.\n",
            "Fitting estimator with 503 features.\n",
            "Fitting estimator with 483 features.\n",
            "Fitting estimator with 463 features.\n",
            "Fitting estimator with 443 features.\n",
            "Fitting estimator with 423 features.\n",
            "Fitting estimator with 403 features.\n",
            "Fitting estimator with 383 features.\n",
            "Fitting estimator with 363 features.\n",
            "Fitting estimator with 343 features.\n",
            "Fitting estimator with 323 features.\n",
            "Fitting estimator with 303 features.\n",
            "Fitting estimator with 283 features.\n",
            "Fitting estimator with 263 features.\n",
            "Fitting estimator with 243 features.\n",
            "Fitting estimator with 223 features.\n",
            "Fitting estimator with 203 features.\n",
            "Fitting estimator with 183 features.\n",
            "Fitting estimator with 163 features.\n",
            "Fitting estimator with 143 features.\n",
            "Fitting estimator with 123 features.\n",
            "Fitting estimator with 103 features.\n",
            "Fitting estimator with 83 features.\n",
            "Fitting estimator with 63 features.\n",
            "Fitting estimator with 43 features.\n",
            "Fitting estimator with 23 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 603 features.\n",
            "Fitting estimator with 583 features.\n",
            "Fitting estimator with 563 features.\n",
            "Fitting estimator with 543 features.\n",
            "Fitting estimator with 523 features.\n",
            "Fitting estimator with 503 features.\n",
            "Fitting estimator with 483 features.\n",
            "Fitting estimator with 463 features.\n",
            "Fitting estimator with 443 features.\n",
            "Fitting estimator with 423 features.\n",
            "Number of features: 403\n",
            "\n",
            "Features:\n",
            " ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card5', 'addr1', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V2', 'V4', 'V5', 'V6', 'V7', 'V10', 'V12', 'V13', 'V19', 'V20', 'V23', 'V24', 'V26', 'V30', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V43', 'V44', 'V45', 'V47', 'V48', 'V49', 'V52', 'V53', 'V54', 'V55', 'V56', 'V58', 'V61', 'V62', 'V64', 'V66', 'V67', 'V70', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V81', 'V82', 'V83', 'V85', 'V86', 'V87', 'V90', 'V91', 'V94', 'V96', 'V99', 'V102', 'V124', 'V126', 'V127', 'V128', 'V129', 'V130', 'V131', 'V133', 'V134', 'V135', 'V136', 'V137', 'V139', 'V140', 'V143', 'V149', 'V150', 'V152', 'V156', 'V160', 'V162', 'V164', 'V165', 'V166', 'V169', 'V170', 'V171', 'V178', 'V184', 'V187', 'V189', 'V197', 'V200', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V214', 'V215', 'V216', 'V217', 'V220', 'V221', 'V222', 'V224', 'V229', 'V232', 'V234', 'V243', 'V245', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V276', 'V277', 'V278', 'V281', 'V282', 'V283', 'V285', 'V291', 'V292', 'V293', 'V294', 'V296', 'V300', 'V303', 'V306', 'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V323', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_08', 'id_09', 'id_11', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_25', 'id_26', 'id_30', 'id_31', 'id_32', 'id_33', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'transaction_month', 'transaction_std', 'transaction_digits', 'transaction_count', 'trans_hour', 'trans_cent', 'prod_stdq', 'card1_count', 'card1_count_q', 'trans_card1_mean', 'trans_card1_mean_rel', 'card1_mean', 'card1_std', 'card1_addr1_mean', 'card2_count', 'card2_out', 'card2_q', 'card4_count', 'trans_card_mean', 'trans_card_mean_q', 'trans_card_std', 'trans_card_std_q', 'card4_trans_mean', 'trans_card_med', 'addr1_frequency', 'addr_trans', 'addr_trans_q', 'addr_card_med', 'addr_card_q', 'addrcard', 'C1_trans_div', 'C1_trans_div_q', 'C2_trans_div', 'C2_trans_div_q', 'C3_trans_div', 'C3_trans_div_q', 'C4_trans_div', 'C4_trans_div_q', 'C5_trans_div', 'C5_trans_div_q', 'C6_trans_div', 'C6_trans_div_q', 'C7_trans_div', 'C7_trans_div_q', 'C8_trans_div', 'C8_trans_div_q', 'C9_trans_div', 'C9_trans_div_q', 'C10_trans_div', 'C10_trans_div_q', 'C11_trans_div', 'C11_trans_div_q', 'C12_trans_div', 'C12_trans_div_q', 'C13_trans_div', 'C13_trans_div_q', 'C14_trans_div', 'C14_trans_div_q', 'TransactionAmt_card1', 'TransactionAmt_card2', 'TransactionAmt_card3', 'TransactionAmt_card4', 'TransactionAmt_card5', 'TransactionAmt_card6', 'TransactionAmt_D15', 'TransactionAmt_C13', 'TransactionAmt_addr1', 'TransactionAmt_dist1', 'TransactionAmt_R_emaildomain', 'TransactionAmt_M2', 'TransactionAmt_M4', 'TransactionAmt_trans_hour', 'card1_card2', 'card1_card3', 'card1_card4', 'card1_card5', 'card1_card6', 'card1_D15', 'card1_C13', 'card1_addr1', 'card1_dist1', 'card1_R_emaildomain', 'card1_M2', 'card1_M4', 'card1_trans_hour', 'card2_card3', 'card2_card4', 'card2_card5', 'card2_card6', 'card2_D15', 'card2_C13', 'card2_addr1', 'card2_dist1', 'card2_R_emaildomain', 'card2_M2', 'card2_M4', 'card2_trans_hour', 'card3_card4', 'card3_card5', 'card3_card6', 'card3_D15', 'card3_C13', 'card3_addr1', 'card3_dist1', 'card3_R_emaildomain', 'card3_M2', 'card3_M4', 'card3_trans_hour', 'card4_card5', 'card4_card6', 'card4_D15', 'card4_C13', 'card4_addr1', 'card4_dist1', 'card4_R_emaildomain', 'card4_M2', 'card4_M4', 'card4_trans_hour', 'card5_card6', 'card5_D15', 'card5_C13', 'card5_addr1', 'card5_dist1', 'card5_R_emaildomain', 'card5_M2', 'card5_M4', 'card5_trans_hour', 'card6_D15', 'card6_C13', 'card6_addr1', 'card6_dist1', 'card6_R_emaildomain', 'card6_M2', 'card6_M4', 'card6_trans_hour', 'D15_C13', 'D15_addr1', 'D15_dist1', 'D15_R_emaildomain', 'D15_M2', 'D15_M4', 'D15_trans_hour', 'C13_addr1', 'C13_dist1', 'C13_R_emaildomain', 'C13_M2', 'C13_M4', 'C13_trans_hour', 'addr1_dist1', 'addr1_R_emaildomain', 'addr1_M2', 'addr1_M4', 'addr1_trans_hour', 'dist1_R_emaildomain', 'dist1_M2', 'dist1_M4', 'dist1_trans_hour', 'R_emaildomain_M2', 'R_emaildomain_M4', 'R_emaildomain_trans_hour', 'M2_M4', 'M2_trans_hour', 'M4_trans_hour']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X5okeU0Q-KI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_before_rfe_step_20 = ['TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card5', 'addr1', 'dist1', 'dist2',\n",
        "                               'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10',\n",
        "                               'C11', 'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11',\n",
        "                               'D12', 'D13', 'D14', 'D15', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V2', 'V4', 'V5', 'V6',\n",
        "                               'V7', 'V10', 'V12', 'V13', 'V19', 'V20', 'V23', 'V24', 'V26', 'V30', 'V34', 'V35', 'V36', 'V37',\n",
        "                               'V38', 'V39', 'V40', 'V43', 'V44', 'V45', 'V47', 'V48', 'V49', 'V52', 'V53', 'V54', 'V55', 'V56',\n",
        "                               'V58', 'V61', 'V62', 'V64', 'V66', 'V67', 'V70', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79',\n",
        "                               'V81', 'V82', 'V83', 'V85', 'V86', 'V87', 'V90', 'V91', 'V94', 'V96', 'V99', 'V102', 'V124', 'V126',\n",
        "                               'V127', 'V128', 'V129', 'V130', 'V131', 'V133', 'V134', 'V135', 'V136', 'V137', 'V139', 'V140', 'V143',\n",
        "                               'V149', 'V150', 'V152', 'V156', 'V160', 'V162', 'V164', 'V165', 'V166', 'V169', 'V170', 'V171', 'V178',\n",
        "                               'V184', 'V187', 'V189', 'V197', 'V200', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209',\n",
        "                               'V210', 'V212', 'V214', 'V215', 'V216', 'V217', 'V220', 'V221', 'V222', 'V224', 'V229', 'V232', 'V234',\n",
        "                               'V243', 'V245', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V263', 'V264', 'V265',\n",
        "                               'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V276', 'V277', 'V278', 'V281', 'V282',\n",
        "                               'V283', 'V285', 'V291', 'V292', 'V293', 'V294', 'V296', 'V300', 'V303', 'V306', 'V307', 'V308', 'V309',\n",
        "                               'V310', 'V311', 'V312', 'V313', 'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V323',\n",
        "                               'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_08', 'id_09', 'id_11', 'id_13', 'id_14', 'id_15',\n",
        "                               'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_25', 'id_26', 'id_30', 'id_31', 'id_32', 'id_33',\n",
        "                               'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'transaction_month', 'transaction_std',\n",
        "                               'transaction_digits', 'transaction_count', 'trans_hour', 'trans_cent', 'prod_stdq', 'card1_count',\n",
        "                               'card1_count_q', 'trans_card1_mean', 'trans_card1_mean_rel', 'card1_mean', 'card1_std', 'card1_addr1_mean',\n",
        "                               'card2_count', 'card2_out', 'card2_q', 'card4_count', 'trans_card_mean', 'trans_card_mean_q', 'trans_card_std',\n",
        "                               'trans_card_std_q', 'card4_trans_mean', 'trans_card_med', 'addr1_frequency', 'addr_trans', 'addr_trans_q',\n",
        "                               'addr_card_med', 'addr_card_q', 'addrcard', 'C1_trans_div', 'C1_trans_div_q', 'C2_trans_div', 'C2_trans_div_q',\n",
        "                               'C3_trans_div', 'C3_trans_div_q', 'C4_trans_div', 'C4_trans_div_q', 'C5_trans_div', 'C5_trans_div_q',\n",
        "                               'C6_trans_div', 'C6_trans_div_q', 'C7_trans_div', 'C7_trans_div_q', 'C8_trans_div', 'C8_trans_div_q',\n",
        "                               'C9_trans_div', 'C9_trans_div_q', 'C10_trans_div', 'C10_trans_div_q', 'C11_trans_div', 'C11_trans_div_q',\n",
        "                               'C12_trans_div', 'C12_trans_div_q', 'C13_trans_div', 'C13_trans_div_q', 'C14_trans_div', 'C14_trans_div_q',\n",
        "                               'TransactionAmt_card1', 'TransactionAmt_card2', 'TransactionAmt_card3', 'TransactionAmt_card4',\n",
        "                               'TransactionAmt_card5', 'TransactionAmt_card6', 'TransactionAmt_D15', 'TransactionAmt_C13', 'TransactionAmt_addr1',\n",
        "                               'TransactionAmt_dist1', 'TransactionAmt_R_emaildomain', 'TransactionAmt_M2', 'TransactionAmt_M4', 'TransactionAmt_trans_hour',\n",
        "                               'card1_card2', 'card1_card3', 'card1_card4', 'card1_card5', 'card1_card6', 'card1_D15', 'card1_C13', 'card1_addr1',\n",
        "                               'card1_dist1', 'card1_R_emaildomain', 'card1_M2', 'card1_M4', 'card1_trans_hour', 'card2_card3', 'card2_card4',\n",
        "                               'card2_card5', 'card2_card6', 'card2_D15', 'card2_C13', 'card2_addr1', 'card2_dist1', 'card2_R_emaildomain', 'card2_M2',\n",
        "                               'card2_M4', 'card2_trans_hour', 'card3_card4', 'card3_card5', 'card3_card6', 'card3_D15', 'card3_C13', 'card3_addr1',\n",
        "                               'card3_dist1', 'card3_R_emaildomain', 'card3_M2', 'card3_M4', 'card3_trans_hour', 'card4_card5', 'card4_card6', 'card4_D15',\n",
        "                               'card4_C13', 'card4_addr1', 'card4_dist1', 'card4_R_emaildomain', 'card4_M2', 'card4_M4', 'card4_trans_hour', 'card5_card6',\n",
        "                               'card5_D15', 'card5_C13', 'card5_addr1', 'card5_dist1', 'card5_R_emaildomain', 'card5_M2', 'card5_M4', 'card5_trans_hour',\n",
        "                               'card6_D15', 'card6_C13', 'card6_addr1', 'card6_dist1', 'card6_R_emaildomain', 'card6_M2', 'card6_M4', 'card6_trans_hour',\n",
        "                               'D15_C13', 'D15_addr1', 'D15_dist1', 'D15_R_emaildomain', 'D15_M2', 'D15_M4', 'D15_trans_hour', 'C13_addr1', 'C13_dist1',\n",
        "                               'C13_R_emaildomain', 'C13_M2', 'C13_M4', 'C13_trans_hour', 'addr1_dist1', 'addr1_R_emaildomain', 'addr1_M2', 'addr1_M4',\n",
        "                               'addr1_trans_hour', 'dist1_R_emaildomain', 'dist1_M2', 'dist1_M4', 'dist1_trans_hour', 'R_emaildomain_M2', 'R_emaildomain_M4',\n",
        "                               'R_emaildomain_trans_hour', 'M2_M4', 'M2_trans_hour', 'M4_trans_hour']\n",
        "\n",
        "                              "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}